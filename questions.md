# Questions

## 一、 特征工程

### 1. 为什么需要对数值类型的特征做归一化？

不同维度的数据的范围不同，所以导致收敛速度不同。而学习率的设置是相同的，但在不同数据维度的下降速度(梯度)不同，导致收敛震荡。
通过梯度下降法求解的模型通常是需要归一化的，包括`线性回归`、`逻辑回归`、`支持向量机`、`神经网络模型`；对于**决策树模型不适用**，因为节点分裂时主要依赖数据集关于特征x的信息增益比。
如下图所示的数据分布，未归一化的数据分布更加狭长，而归一化数据的梯度像碗一样，分布更加均匀。

### 2. 怎样处理类别特征？

* 序号编码(Ordinal Encoding)
用于处理类别间具有大小关系的数据，如`高>中>低`转换为`2>1>0`，仍保留大小关系

* 独热编码(One-hot Encoding)
用于处理类别之间不具有大小关系的特征，如血型，把A型转换为(1, 0, 0, 0)的稀疏矩阵。需要注意以下问题：
  
  * 节省空间，算法输入使用稀疏矩阵。
  * 配合特征选择降低维度，原因如下：
    * 高维情况下两点距离很难得到有效衡量。`K近邻`
    * 参数数量过多，容易过拟合。`逻辑回归`
    * 通常只有部分维度对模型学习有帮助。

* 二进制编码(Binary Encoding)
给每个类别赋予一个ID，得到该ID的二进制表示作为特征映射，即相应属性下只有**0/1**取值。

### 3. 什么是组合特征？如何处理高维组合特征？

#### 组合特征

在分类预测任务中中，特征通常都不是单独作用于结果的，而是把一阶离散特征两两组合构成高阶组合特征。

#### 处理高维组合特征
将两个特征合并为同一个特征，例如指定(用户ID=1并且物品ID=1)为一个属性。但容易引起组合爆炸，使参数数量过多，如用户数量为$m$，物品数量为$n$，几乎无法学习$m \times n$个参数。一种方法是将用户和物品分别用k维的向量表示，$k \ll m, k \ll n$，需要学习的参数为$m \times k + n \times k$，等价于推荐系统的矩阵分解。

#### *矩阵分解

### 4. 怎样有效地找到组合特征？

基于决策树的方法，可以使用集成学习算法中的**梯度提升决策树(GBDT)**，找出特征的有效分裂点，和与其他特征之间的关系。

### 5. 有哪些文本表示模型？它们各有什么优缺点？

* 词袋模型(BOW)和N-gram模型
  词袋模型忽略了词出现的顺序，输入的文章转换为一个长向量，每一维代表一个单词，该维对应的权重反映了这个词在原文章中的重要程度，通常用TF-IDF来计算。
  $$
    TF-IDF(t, d) = TF(t, d) \times IDF(t) \\
    IDF(t) = log\frac{DocumentsCount}{包含单词t的文章总数+1}
  $$
  N-gram模型可以将多个单词连起来的固定搭配作为一维特征进行输入，如("自然", "语言", "处理")的合并单独作为一个特征(Word Piece)，英文有时还需要进行词干(word stemming)抽取处理。
* 主题模型
  将大量文章进行无监督地聚类学习，发现代表性的主题和每个主题上面有代表性的词，从而计算出每篇文章的主题分布，主要方法有LDA(隐狄利克雷模型)。
* 词嵌入与深度学习
  WordEmbedding，是一种概率语言模型，能够将词映射到低维空间(通常50～300)。由于传统的模型更加稀疏，深度学习无法很好地处理这种数据，word2vec是谷歌的一个词嵌入实现，将深度学习引入NLP领域成为了可能。

## TODO LIST

* [ ] 添加图片
* [ ] 完成所有问题输入及解答
* [ ] pandas、sklearn示例